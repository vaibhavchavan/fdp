{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c23357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "844/844 [==============================] - 27s 30ms/step - loss: 0.1958 - accuracy: 0.9410 - val_loss: 0.0583 - val_accuracy: 0.9830\n",
      "Epoch 2/5\n",
      "844/844 [==============================] - 24s 29ms/step - loss: 0.0548 - accuracy: 0.9832 - val_loss: 0.0434 - val_accuracy: 0.9885\n",
      "Epoch 3/5\n",
      "844/844 [==============================] - 24s 29ms/step - loss: 0.0368 - accuracy: 0.9883 - val_loss: 0.0412 - val_accuracy: 0.9877\n",
      "Epoch 4/5\n",
      "844/844 [==============================] - 24s 29ms/step - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.0336 - val_accuracy: 0.9905\n",
      "Epoch 5/5\n",
      "844/844 [==============================] - 23s 27ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.0350 - val_accuracy: 0.9905\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0226 - accuracy: 0.9931\n",
      "Test accuracy: 0.9930999875068665\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "#train_images: Contains 60,000 images used for training the model. Shape: (60000, 28, 28).\n",
    "#train_labels: Contains the corresponding labels (actual digits) for each training image. Shape: (60000,).\n",
    "\n",
    "#test_images: Contains 10,000 images used for testing the model after training.\n",
    "#Shape: (10000, 28, 28).\n",
    "#test_labels: Contains the corresponding labels for each test image. \n",
    "#Shape: (10000,).\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "#The .reshape() method changes the shape of the train_images array without \n",
    "#altering its data.\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "#Dividing by 255.0 scales all pixel values to the range [0, 1]:\n",
    "#Example: A pixel with a value of 128 becomes 128 / 255.0 ≈ 0.50196.\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential() \n",
    "#build a sequential neural network model. A Sequential model is a linear stack \n",
    "#of layers \n",
    "#where you can define a model layer-by-layer in a specific sequence.\n",
    "\n",
    "# First Convolutional Layer\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "#MaxPooling is a pooling operation commonly used in convolutional neural \n",
    "#networks (CNNs) to reduce \n",
    "#the spatial dimensions (width and height) of feature maps. \n",
    "#It helps in dimensionality reduction while retaining the most important features in an image.\n",
    "\n",
    "# Second Convolutional Layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third Convolutional Layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Flatten the output from 2D to 1D\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(layers.Dense(10, activation='softmax'))  # 10 classes for digits 0-9\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6a2260",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "    Data Preprocessing:\n",
    "        The MNIST dataset is loaded and reshaped to fit the CNN input shape (28, 28, 1).\n",
    "        The pixel values are normalized to the range [0, 1] by dividing by 255.\n",
    "        The labels are one-hot encoded, which converts them from integer labels (0-9) to a binary matrix.\n",
    "\n",
    "    CNN Model:\n",
    "        Three convolutional layers are used with ReLU activation functions.\n",
    "        Max-pooling layers are applied after each convolution to reduce the spatial dimensions.\n",
    "        The output is flattened into a 1D vector to feed into a fully connected layer.\n",
    "        The final output layer uses a softmax activation to predict the class probabilities.\n",
    "\n",
    "    Model Compilation & Training:\n",
    "        The model is compiled using the Adam optimizer and categorical cross-entropy loss (appropriate for multi-class classification).\n",
    "        The model is trained for 5 epochs, and the test accuracy is printed at the end.\n",
    "\n",
    "This is a basic CNN architecture and can be improved with more layers, data augmentation, or dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e25c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "844/844 [==============================] - 25s 28ms/step - loss: 0.1942 - accuracy: 0.9415 - val_loss: 0.0782 - val_accuracy: 0.9757\n",
      "Epoch 2/5\n",
      "844/844 [==============================] - 23s 28ms/step - loss: 0.0565 - accuracy: 0.9822 - val_loss: 0.0415 - val_accuracy: 0.9888\n",
      "Epoch 3/5\n",
      "844/844 [==============================] - 25s 30ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0370 - val_accuracy: 0.9897\n",
      "Epoch 4/5\n",
      "844/844 [==============================] - 26s 31ms/step - loss: 0.0305 - accuracy: 0.9901 - val_loss: 0.0435 - val_accuracy: 0.9870\n",
      "Epoch 5/5\n",
      "844/844 [==============================] - 25s 30ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0427 - val_accuracy: 0.9887\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0314 - accuracy: 0.9898\n",
      "Test accuracy: 0.989799976348877\n",
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Display a prediction example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first test image and its predicted label\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Predicted: {np.argmax(predictions[0])}, True: {np.argmax(test_labels[0])}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8580758d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "    Dataset:\n",
    "        The MNIST dataset contains grayscale images of handwritten digits with dimensions (28, 28).\n",
    "\n",
    "    Preprocessing:\n",
    "        Images are reshaped to (28, 28, 1) to include a channel dimension for the CNN.\n",
    "        Pixel values are normalized to the range [0, 1] by dividing by 255.\n",
    "        Labels are one-hot encoded (e.g., 3 → [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]).\n",
    "\n",
    "    CNN Architecture:\n",
    "        Conv2D Layers: Extract features from the images using 3x3 filters.\n",
    "        MaxPooling2D: Reduces the spatial dimensions, retaining important features.\n",
    "        Flatten: Converts the 2D feature maps into a 1D vector for the dense layers.\n",
    "        Dense Layers: Fully connected layers for learning and classification.\n",
    "\n",
    "    Model Compilation:\n",
    "        Optimizer: Adam for adaptive learning rates.\n",
    "        Loss: Categorical cross-entropy for multi-class classification.\n",
    "        Metrics: Accuracy to evaluate performance.\n",
    "\n",
    "    Training:\n",
    "        The model is trained for 5 epochs with a batch size of 64.\n",
    "        Validation data is taken as 10% of the training data.\n",
    "\n",
    "    Testing:\n",
    "        The model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "    Predictions:\n",
    "        The model's predictions are shown alongside the true labels for the first test image.\n",
    "\n",
    "This is a straightforward example of training and testing a CNN. It can be expanded with data augmentation or deeper architectures for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567c907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
